---
title:
author:
date:
header-includes:
    - \usepackage{setspace}
output: pdf_document
---

```{r, include = FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

\title{\large Support Vector Machines \& and an Application to Handwriting Analysis\\}
\author{Chandler Zachary \\ MATH 6388 Final Project \\ CU Denver \\}
\date{\small Submitted \today}
\maketitle
\thispagestyle{empty}
\clearpage
\setcounter{page}{1}
\doublespacing

# Introduction

Since their introduction by Vladimir Vapnik in 1995, support vector machines ("SVMs") have earned a reputation as a superb "out-of-the-box" classifier.

My treatment of the mathematical exposition of hyperplanes and support vectors follows closely the original development in Cortes and Vapnik (1995) and the subsequent Burges (1998). Furthermore, like Cortes and Vapnik (1995), I apply support vector machines to the handwrittten zip code data, although my application differs in two ways: I use a subset of the data, and I apply a radial basis kernel.

The remainder of this paper is constructed as follows: the next section discusses the mathematics behind SVMs followed by an application of SVMs to the handwritten zip code data. A conclusion concludes the paper.

# Method

Support vector machines are in the category of *discriminative* classifiers which separate classes based on a decision boundary. This style includes classifiers such as linear and quadratic discriminant and logistic regression. They are distinguished from *generative* classifiers which separate classes based on the distributions of the classes themselves and which include classifiers such as k-means. SVMs estimate decision boundaries called *separating hyperplanes*. [Add some stuff here to compare and contrast LDA and logistic with SVMs.] I develop the mathematics in three stages, beginning with an introduction to hyperplanes, moving to support vector classifiers, and finally SVMs.

### Separating Hyperplanes & Maximal Margin Classifiers

##### Separable Case

Support vector machines classify observations according to decision boundaries called *separating hyperplanes*. A separating hyperplane is a flat, affine *(p-1)*-dimensional subspace of a *p*-dimensional predictor space. In the simplest case to visualize, a separting hyperplane of a two-dimensional predictor space is a line, and observations are classified according to which side of the line they lie. Affine subspaces of three-dimensional space are planes, and those of four-dimensional space are surfaces. In high dimensional predictor spaces, visualizing a hyperplane is not so simple. 

Momentarily abstracting away from any reference to data or predictor spaces for the sake of exposition, for a *p*-dimensional space, a hyperplane can be defined by the following equation:

\begin{equation} \label{eq:1}
\{\textbf{x} | f(\textbf{x}) = a + \textbf{w}^T \textbf{x} = 0\},
\end{equation}

where $\textbf{x}$ is a vector of length *p-1*, and $\textbf{w}^T$ are the respective weights for the points in $\textbf{x}$; *a* is a constant. An observation $\textbf{x} = (x_1, x_2, ..., x_{p-1})^T$ lies on either side of the hyperplane based upon whether the value of $\hat{f}(\textbf{x})$ is positive or negative. This is the foundation of classification using SVMs: assuming a hyperplane exists for the training data, a classifier can be constructed that classifies observations based upon the sign of $\hat{f}(\textbf{x})$.

Separating hyperplanes are estimated by maximizing the *margin*, which is the smallest perpendicular distance between training observations and the hyperplane. A linear classifier for a two-class response is known as the *maximal margin classifier* because observations are classified by whether they lie further away from the hyperplane than the maximum margin for their respective class. Thus the classifier is constructed using both the sign and the magnitude of $\hat{f}(\textbf{x})$. If the margins in the test data are similar to those of the training data, then the decision boundary will have a low misclassification rate.

### From Maximal Margin Classifiers to Support Vector Classifiers

The maximal margin classifier (also called the *optimal separating hyperplane*) generally solves the following optimization problem:

\begin{equation} \label{eq:2}
\begin{split}
& \max_{b, \textbf{w}, \|{\textbf{w}}\|=1} \quad M\\
& subject \: to \quad y_{i} (b + \textbf{x}_i^T\textbf{w}) \geq M, \: i = 1,..., N.\\
\end{split}
\end{equation}

The term *M* represents the margin as described above. I now show how this problem can be rewritten as a convex optimization problem. This is desirable because of the advantages of convex optimization. [Cite the Rockafellar paper here.] I will also show how the problem can be solved for linearly separable classes and thereby lay the groundwork for building the support vector machine.

Consider a training data set $(y_i, \textbf{x}_i), i \in \{1, ..., k\}, \; y_i \in \{-1, 1\}, \; \textbf{x}_i \in \mathbb{R}^p$. If \textbf{w} and *a* exist such that

\begin{equation} \label{eq:3}
\begin{split}
& a + \textbf{w} \cdot \textbf{x}_i \geq 1 \quad if \quad y_i = 1\\
& a + \textbf{w} \cdot \textbf{x}_i \leq 1 \quad if \quad y_i = -1\\
\end{split}
\end{equation}

are satisfied, then the training data are linearly separable. The inequalities in (\ref{eq:3}) can be rewritten to resemble the constraint in (\ref{eq:2}):

\begin{equation} \label{eq:4}
y_{i} (b + \textbf{x}_i^T\textbf{w}) \geq 1, \; i = 1,..., k
\end{equation}

Observe that three distinct hyperplanes can be defined, and all three are necessary for constructing the maximal margin classifier:

\begin{equation} \label{eq:5}
\begin{split}
& a + \textbf{w} \cdot \textbf{x}_i = 1\\
& a + \textbf{w} \cdot \textbf{x} = 0\\
& a + \textbf{w} \cdot \textbf{x}_i = -1\\
\end{split}
\end{equation}

The second of these hyperplanes is the hyperplane which linearly separates the classes in the training data, and the first and third of these hyperplanes define the margin. The maximal margin classifier maximizes the perpendicular distance between the separating hyperplane and the margin. The perpendicular distances between the margin and the hyperplane can be given by $|1-a|/||\textbf{w}||$ and $|-1-a|/||\textbf{w}||$, where $||\textbf{w}||$ is the Euclidean distance. Therefore, the distance between each of the margin hyperplanes and the separating hyperplane is given by $1/||\textbf{w}||$, and the total width of the margin is $2/||\textbf{w}|| = M$. Minimizing this total width is given by $\min_{a, \textbf{w}} \cfrac{1}{2} \textbf{w}^{T}\textbf{w}$. Therefore, the optimization problem (\ref{eq:2}) can be rewritten as

\begin{equation} \label{eq:6}
\begin{split}
& \min_{b, \textbf{w}} \cfrac{1}{2} \textbf{w}^{T}\textbf{w}\\
& subject \: to \quad y_{i} (b + \textbf{x}_i^T\textbf{w}) \geq 1, \: i = 1,..., k\\
\end{split}
\end{equation}

The Lagrangian for this minimization problem is:

\begin{equation} \label{eq:7}
\mathcal{L} = \cfrac{1}{2} \textbf{w}^{T}\textbf{w} - \sum_{i=l}^{l} \alpha_i (y_i(\textbf{x}_i \cdot \textbf{w} + b) - 1),
\end{equation}

where each $\alpha_i$ is an inequality constraint from (\ref{eq:6}), and the first-order conditions for the Lagrangian can be written succinctly as:

\begin{equation} \label{eq:8}
\begin{split}
& \cfrac{\partial\mathcal{L}}{\partial{\textbf{w}}} = \textbf{w} - \sum_{i=l}^{l} \alpha_i y_i \textbf{x}_i = 0\\
& \cfrac{\partial\mathcal{L}}{\partial b} = \sum_{\alpha_i} \alpha_i y_i = 0.\\
\end{split}
\end{equation}

Following optimization procedure, the first condition in (\ref{eq:8}) is solved for $\textbf{w}$, and these conditions are then substituted into (\ref{eq:7}):

\begin{equation} \label{eq:9}
\mathcal{L} = \sum_{i} \alpha_i - \cfrac {1}{2} \sum_{i,j} \alpha_i \alpha_j y_i y_j \textbf{x}_i \cdot \textbf{x}_j.
\end{equation}

This is the advantage of reproducing (\ref{eq:2}) as a convex optimization problem: minimizing (\ref{eq:7}) is identical to maximizing (\ref{eq:9}) subject to the constraints that the derivatives with respect to $\alpha_i$ vanish and $\alpha_i \geq 0$, and the optimization will produce identical solutions. The formulation in (\ref{eq:7}) is called the *Lagrange primal*, denoted *L~P~*, and the formulation in (\ref{eq:9}) is called the *Wolfe dual*, denoted *L~D~*. The Wolfe dual maximization problem is easier to solve and has fruitful implications for the *kernel trick* described in the next section when using SVMs for non-linear relationships.

The solution this optimization problem is the result of solving the first condition in (\ref{eq:8}) for $\textbf{w}$, which I reproduce for reference:

\begin{equation} \label{eq:10}
\hat{\textbf{w}} = \sum_{i} \hat{\alpha}_i y_i \textbf{x}_i.
\end{equation}


This is an affine subspace of the original predictor space: a hyperplane. Under the Kuhn-Tucker Theorem adds an additional constraint for *L~P~* stated above, which allows us to rewrite the solution in a familiar way and to add some helpful intuition:

\begin{equation} \label{eq:11}
\alpha_i [y_i(b + \textbf{x}_i^T\textbf{w}) - 1] = 0, \quad \forall i.
\end{equation}

This additional constraint demonstrates two things. First, observe that the optimal separating hyperplane can be derived from this condition by noting that the *y~i~* and \textbf{x}_i^T are test and training observations. The optimal separating hyperplane is the portion of the condition defined by:

\begin{equation}
\hat{f}(\textbf{x}) = \hat{b} + \textbf{x}_i^T \hat{\textbf{w}}.
\end{equation}

Second, having defined the optimal separating hyperplane, we see that $\hat{\alpha}_i$ determines which $\textbf{x}_i^T$ are actually relevant in determining the optimal separating hyperplane. Non-zero values of $\hat{\alpha}_i$ occur only for the points $\textbf{x}_i^T$ for which $y_i(b + \textbf{x}_i^T) = 1$, and these are the points *on the margin M*. When $\hat{\alpha}_i = 0$, this condition is not satisfied, and these are points $\textbf{x}_i^T$ far away from the margin. This is also the origin of the term *support vector*, which is a point $\textbf{x}_i^T$ lying on the margin. It merits attention that the elegance of the hyperplane rests on being defined by what it is *not*: the separating hyperplane exists exactly in the center of a corridor where there can be no observations.

##### Non-Separable Case

When the classes are non-separable such that observations from different classes are comingled, the algorithm must be modified by relaxing the inequality constraints in (\ref{eq:6}). This is done via *slack* variables $\xi \geq 0, i = 1, ..., k$. The optimization problem then becomes:

\begin{equation} \label{eq:13}
\begin{split}
& \min_{b, \textbf{w}} \cfrac{1}{2} \textbf{w}^{T}\textbf{w} + c\sum_{i=1}^{k}\xi_i\\
& subject \: to \quad y_{i} (b + \textbf{x}_i^T\textbf{w}) \geq M(1-\xi), \quad \xi_i \geq 0, \quad \forall i \in \{1, ...,k\}.\\
\end{split}
\end{equation}

The role of the $\xi_i$ is directly tied to the effect of misclassification. The maximal margin hyperplane *perfectly* separates classes and is thus prone to overfitting. To add flexibility to the model, we want it to be wrong sometimes. That is to say, we want to capture the instances where $\hat{f}(\textbf{x})$ is on the wrong side of the hyperplane. This happens when $\xi_i > 1$, and the term $\sum{\xi_i}$ bounds the number of times $\hat{f}(\textbf{x})$ is allowed to be on the wrong side. The constant *c* is a *cost* parameter that defines the width of the margin and has the effect of assigning a cost to violating that margin. A small cost parameter assigns a wide margin which means potentially more support vectors violating the margin, and a large cost parameter assigns a narrow margin which means fewer support vectors violating the margin.

This algorithm is called the *soft margin hyperplane* or the *support vector classifier*, and it is the second step in formulating the support vector machine. It defines a linear boundary for classifying non-separable data. The advantage of the support vector classifier is its flexibility in defining a *nearly* separating hyperplane for the cases where (\ref{eq:2}) has no solution with $M > 0$. It uses only the observations that lie on or that violate the margin to define the hyperplane. This highlights the role of the parameter *c*: it can be thought of as moderating the bias-variance trade-off for the the soft margin. Larger values of *c* expand the number of observations that determine the hyperplane, which means low variance but large bias, and smaller values of *c* restrict the number of observations that determine the hyperplane, which means high variance but low bias. Furthermore, the support vector classifier uses only those observations that are at or across the margin and boundary, thus mitigating the effects of observations further away.

Solving problem (\ref{eq:13}) is analogous to solving for the optimal separating hyperplane. First, state the Lagrange primal function and solve its first order conditions to create the Wolfe dual, then apply the Kuhn-Tucker Theorem. The Lagrange primal is:

\begin{equation} \label{eq:14}
\mathcal{L}_P = \cfrac{1}{2} \textbf{w}^{T}\textbf{w} + c\sum_{i=1}^{k}\xi_i - \sum_{i=l}^{k} \alpha_i [y_i(\textbf{x}_i \cdot \textbf{w} + b) - (1-\xi_i)] - \sum_{i=1}^{l}\mu_i\xi_i,
\end{equation}

where $\mu_i$ ensures $\xi_i \geq 0$. The first order conditions are:

\begin{equation} \label{eq:15}
\begin{split}
&\cfrac {\partial \mathcal{L}}{\partial \textbf{w}} = \textbf{w} - \sum_{i} \alpha_i y_i \textbf{x}_i = 0\\
&\cfrac {\partial \mathcal{L}}{\partial b} = \sum_{i} \alpha_i y_i = 0\\
&\cfrac {\partial \mathcal{L}}{\partial \xi} = c - \alpha_i - \mu_i = 0,\\
\end{split}
\end{equation}

and solving these yields,

\begin{equation} \label{eq:16}
\begin{split}
&\textbf{w} = \sum_{i} \alpha_i y_i \textbf{x}_i\\
&\sum_{i} \alpha_i y_i = 0\\
&\alpha_i = c - \mu_i.\\
\end{split}
\end{equation}

Substituting these solutions into the primal gives the Wolfe dual:

\begin{equation} \label{eq:17}
\mathcal{L}_D = \sum_{i=1}^{k} - \cfrac{1}{2} \sum_{i=1}^{k} \sum_{j=1}^{k} \alpha_i\alpha_j y_iy_j \textbf{x}_i^T \cdot \textbf{x}_j,
\end{equation}

which is maximized according to the constraints that $0 \leq \alpha_i \leq c$ and $\sum_{i}\alpha_iy_i = 0$. Additionally, the Karush-Kuhn-Tucker conditions demand:

\begin{equation} \label{eq:18}
\begin{split}
&\alpha_i[y_i(\textbf{x}_i^T\textbf{w} + b) - (1-\xi_i)] = 0\\
&y_i(\textbf{x}_i^T\textbf{w} + b) - (1-\xi_i) \geq 0\\
&\mu_i\xi_i = 0,\\
\end{split}
\end{equation}

and the solution is given by:

\begin{equation} \label{eq:19}
\hat{\textbf{w}} = \sum_{i} \hat{\alpha}_i y_i \textbf{x}_i.
\end{equation}

Here again, only non-zero $\hat{\alpha}_i$ pick out the support vectors. Observe the role of $\hat{\xi}_i$ and *c* in this solution: together, they determine the soft margin. Based on the Karush-Kuhn-Tucker conditions (\ref{eq:18}), whenever $\hat{\xi}_i = 0$, more or fewer $\textbf{x}_i^T$ are allowed to lie on or violate the margin according to the third constraing given in (\ref{eq:16}).

### Support Vector Machines

The previous discussion of maximal margin and support vector classifiers does most of the work in developing the support vector machine. Until now, classifiers have been defined based on linear relationships in the data. However, relationships are not always linear. Moreover, one may not always be able to make an informed decision *a priori* about whether the relationships in the data are linear or non-linear. The so-called *kernel trick* is the final building block that extends the previous classifiers to non-linear decision boundaries and add greater flexibility. This deceptively simple addition gives SVMs properties that can easily tackle a wide range of classification problems.

Consider a mapping $\Phi: \mathbb{R}^p \rightarrow \mathbb{R}^q$, with $p < q$, that expands the input space into larger dimensions. Once the input space is expanded, approximately linear boundaries can be found that separate training observations. In the expanded space, the data become $\Phi(\textbf{x}) = \Phi_1(\textbf{x}_i), ..., \Phi_1(\textbf{x}_i), i = 1, ..., l$. An observation is then classified by the function

\begin{equation} \label{eq:20}
f(\textbf{x}) = b + \textbf{w} \cdot \Phi(\textbf{x}),
\end{equation}

where

\begin{equation}
\textbf{w} = \sum_{i=1}^{l} \alpha_i y_i \Phi(\textbf{x}_i)
\end{equation}

solves the optimization in the expanded space. The "trick" in the kernel trick is to define a kernel $K(\textbf{x}_i, \textbf{x}_j) = \Phi(\textbf{x}_i) \cdot \Phi(\textbf{x}_j)$ that generates support vectors in the expanded input space without requiring an explicit definition of $\Phi$. The mapping $\Phi$ can be any combination of basis functions, but one does not need to know what those functions are because the kernel does the heavy lifting. The kernel can do the heavy lifting because dot products are linear. The affine subspaces that solve the optimization problems are dot products, and $f(\textbf{x})$ in (\ref{eq:20}) is determined by those dot products. Thus we can write:

\begin{equation} \label{eq:22}
f(\textbf{x}) = b + \textbf{w} \cdot \Phi(\textbf{x}) = b + \sum_{i=1}^{l} \alpha_i y_i \Phi(\textbf{x}_i) \cdot \Phi(\textbf{x})
\end{equation}

Since a kernel is defined to be $K(\textbf{x}_i, \textbf{x}_j) = \Phi(\textbf{x}_i) \cdot \Phi(\textbf{x}_j)$, (\ref{eq:22}) becomes

\begin{equation} \label{eq:23}
f(\textbf{x}) = b + \sum_{i=1}^{l} \alpha_i y_i \Phi(\textbf{x}_i) \cdot \Phi(\textbf{x}) = b + \sum_{i=1}^{l} \alpha_i y_i K(\textbf{x}_i, \textbf{x}),
\end{equation}

where the $\textbf{x}_i$ are the support vectors. Thus reveals the simplicity of the kernel trick. As long as $\mathbb{R}^p$ and $\mathbb{R}^q$ both are dot product spaces whose norms satisfy Cauchy convergence, there exists a mapping $\Phi$ that can transform the original non-linear space into a linear space of higher dimensions. Moreover, specification of $\Phi$ is not required because the kernel maps the transformed data to the classifier $f(\textbf{x})$. I refer the reader to either of Cortes and Vapnik (1995) or Burges (1998) for details on Mercer's condition which $K(\textbf{x}_i, \textbf{x}_j) = \Phi(\textbf{x}_i) \cdot \Phi(\textbf{x}_j)$ must satisfy.

The classifier for a support vector machine is given by:

\begin{equation} \label{eq:23}
\hat{f}(\textbf{x}) = \hat{b} + \sum_{i}\hat{\alpha}_i y_i K(\textbf{x}_i, \textbf{x}).
\end{equation}

There are several popular kernels for SVMs, three of which I reproduce here. The polynomial, radial basis, and neural network kernels, respectively:

\begin{equation}
\begin{split}
&K(\textbf{x}, \textbf{y}) = (1 + \textbf{x} \cdot \textbf{y})^{k}\\
&K(\textbf{x}, \textbf{y}) = e^{-||\textbf{x} - \textbf{y}||^{2}/2\sigma^{2}}\\
&K(\textbf{x}, \textbf{y}) = tanh(\kappa \textbf{x} \cdot \textbf{y} - \delta)\\
\end{split}
\end{equation}

In the expanded input space, optimally separating hyperplanes can be estimated over linear boundaries that are non-linear in the original input space. For example, the radial kernel in reduced dimensions may appear to produce a circular decision boundary with a circular margin. The polynomial kernel may look like a map of traffic patterns. In the expanded inpute space, $\hat{\xi}_i$ and *c* perform the same roles as in the soft margin classifier to moderate the number of observations that are allowed to violate the margin.

# Application

### Data

The data are 16-pixel by 16-pixel images of handwritten numbers scanned from zip codes on envelopes by the United States Postal Service. Each row of data is a digit from 0 to 9, followed by 256 grayscale values, which represent the amount of gray in a pixel. The data are separated into training and test sets, with 7,291 observations and 2,007 observations, respectively.

```{r message = FALSE}
rm(list=ls())
setwd("E:\\MATH\\MATH 6388 -- Machine Learning\\Projects\\Final Project")
#install.packages(e1071)

zipTrain = read.table("zip.train")
zipTest = read.table("zip.test")
```

\begin{center}
\textbf{Images of Numbers from Training Data}
\end{center}
```{r}
## plot a few images
im0 <- matrix(as.numeric(zipTrain[9, 2:257]), nrow = 16, ncol = 16)
im1 <- matrix(as.numeric(zipTrain[8, 2:257]), nrow = 16, ncol = 16)
im2 <- matrix(as.numeric(zipTrain[42, 2:257]), nrow = 16, ncol = 16)
im3 <- matrix(as.numeric(zipTrain[5, 2:257]), nrow = 16, ncol = 16)
im4 <- matrix(as.numeric(zipTrain[4, 2:257]), nrow = 16, ncol = 16)
im5 <- matrix(as.numeric(zipTrain[2, 2:257]), nrow = 16, ncol = 16)
im6 <- matrix(as.numeric(zipTrain[1, 2:257]), nrow = 16, ncol = 16)
im7 <- matrix(as.numeric(zipTrain[4, 2:257]), nrow = 16, ncol = 16)
im8 <- matrix(as.numeric(zipTrain[18, 2:257]), nrow = 16, ncol = 16)
im9 <- matrix(as.numeric(zipTrain[65, 2:257]), nrow = 16, ncol = 16)

png("number_images.png")
par(mfrow=c(2, 5))
image(t(apply(-im0, 1, rev)), col = gray((0:32)/32))  ## this is a 0
image(t(apply(-im1, 1, rev)), col = gray((0:32)/32))  ## this is a 1
image(t(apply(-im2, 1, rev)), col = gray((0:32)/32))  ## this is a 2
image(t(apply(-im3, 1, rev)), col = gray((0:32)/32))  ## this is a 3
image(t(apply(-im4, 1, rev)), col = gray((0:32)/32))  ## this is a 4
image(t(apply(-im5, 1, rev)), col = gray((0:32)/32))  ## this is a 5
image(t(apply(-im6, 1, rev)), col = gray((0:32)/32))  ## this is a 6
image(t(apply(-im7, 1, rev)), col = gray((0:32)/32))  ## this is a 7
image(t(apply(-im8, 1, rev)), col = gray((0:32)/32))  ## this is a 8
image(t(apply(-im9, 1, rev)), col = gray((0:32)/32))  ## this is a 9
dev.off()
```

\singlespacing
\begin{center}
\textbf{Distribution of Digits in Training and Test Data}
\end{center}
|        |  0  |  1  | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 |
|:------:|:---:|:---:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|
|Training|1,149|1,005|731|658|652|556|664|645|542|644|
|  Test  | 359 | 264 |198|166|200|160|170|147|166|177|
\doublespacing

### Model Selection

Model selection for SVMs is generally best performed using cross-validation[cite Chi Hsu here] and requires specification of a *cost* parameter and, if a kernel is used, one or more kernel-specific parameters.  The radial kernel requires a $\gamma$ parameter. Cross-validation is then conducted in concert with a grid search over specified ranges for cost and $\gamma$. One common method for doing this is to start with a wide grid search and then narrow the range around optimal values. [Some stuff should go here about good starting ranges gamma and cost.]

To give an idea of the effects of different values of cost and $\gamma$, I present the true digits against the predicted digits below for the test data.

\singlespacing

\begin{table}\centering
\caption{Support Vector Classifier}
\begin{tabular}{c c c c c c c c c c c}
\toprule

{} &{0} &{1} &{2} &{3} &{4} &{5} &{6} &{7} &{8} &{9}\\
\hline

{0} &{345} &{0} &{1} &{3} &{2} &{0} &{6} &{0} &{1} &{1}\\
\addlinespace

{1} &{0} &{258} &{0} &{0} &{3} &{0} &{3} &{0} &{0} &{0}\\
\addlinespace

{2} &{5} &{0} &{157} &{6} &{13} &{2} &{4} &{1} &{10} &{0}\\
\addlinespace

{3} &{4} &{0} &{4} &{141} &{0} &{11} &{0} &{1} &{3} &{2}\\
\addlinespace

{4} &{0} &{5} &{7} &{0} &{178} &{0} &{2} &{1} &{1} &{6}\\
\addlinespace

{5} &{8} &{0} &{0} &{9} &{5} &{129} &{1} &{0} &{3} &{5}\\
\addlinespace

{6} &{7} &{0} &{3} &{0} &{3} &{2} &{154} &{0} &{1} &{0}\\
\addlinespace

{7} &{0} &{2} &{0} &{0} &{9} &{0} &{0} &{124} &{1} &{11}\\
\addlinespace

{8} &{6} &{2} &{2} &{16} &{6} &{4} &{0} &{1} &{123} &{6}\\
\addlinespace

{9} &{0} &{3} &{0} &{0} &{5} &{1} &{0} &{3} &{3} &{162}\\

\bottomrule
\end{tabular}
\end{table}

\begin{table}\centering
\begin{tabular}{c c c c c c c c c c c}

\toprule

{} &{0} &{1} &{2} &{3} &{4} &{5} &{6} &{7} &{8} &{9}\\
\hline

{0} &{350} &{0} &{0} &{0} &{4} &{0} &{4} &{0} &{0} &{1}\\
\addlinespace

{1} &{0} &{260} &{0} &{0} &{3} &{0} &{1} &{0} &{0} &{0}\\
\addlinespace

{2} &{14} &{6} &{138} &{11} &{20} &{1} &{3} &{2} &{0} &{3}\\
\addlinespace

{3} &{16} &{3} &{5} &{128} &{2} &{0} &{2} &{1} &{0} &{9}\\
\addlinespace

{4} &{2} &{20} &{3} &{0} &{169} &{0} &{3} &{1} &{0} &{2}\\
\addlinespace

{5} &{55} &{5} &{1} &{40} &{16} &{23} &{10} &{0} &{0} &{10}\\
\addlinespace

{6} &{44} &{2} &{3} &{0} &{3} &{0} &{118} &{0} &{0} &{0}\\
\addlinespace

{7} &{0} &{6} &{1} &{0} &{24} &{0} &{0} &{59} &{0} &{57}\\
\addlinespace

{8} &{27} &{17} &{6} &{20} &{45} &{0} &{3} &{2} &{5} &{41}\\
\addlinespace

{9} &{1} &{14} &{0} &{0} &{36} &{0} &{0} &{1} &{0} &{125}\\

\bottomrule


\toprule

{} &{0} &{1} &{2} &{3} &{4} &{5} &{6} &{7} &{8} &{9}\\
\hline

{0} &{349} &{0} &{1} &{1} &{2} &{0} &{4} &{0} &{1} &{1}\\
\addlinespace

{1} &{0} &{254} &{0} &{0} &{4} &{0} &{5} &{0} &{0} &{1}\\
\addlinespace

{2} &{5} &{0} &{166} &{4} &{9} &{2} &{2} &{1} &{9} &{0}\\
\addlinespace

{3} &{3} &{0} &{7} &{140} &{0} &{12} &{0} &{1} &{1} &{2}\\
\addlinespace

{4} &{0} &{2} &{8} &{0} &{184} &{0} &{1} &{1} &{0} &{4}\\
\addlinespace

{5} &{7} &{0} &{0} &{9} &{3} &{134} &{0} &{1} &{1} &{5}\\
\addlinespace

{6} &{6} &{0} &{4} &{0} &{3} &{3} &{153} &{0} &{1} &{0}\\
\addlinespace

{7} &{0} &{0} &{2} &{0} &{9} &{0} &{0} &{131} &{0} &{5}\\
\addlinespace

{8} &{5} &{2} &{4} &{15} &{2} &{3} &{0} &{1} &{130} &{4}\\
\addlinespace

{9} &{0} &{3} &{0} &{0} &{4} &{1} &{0} &{3} &{3} &{163}\\
\addlinespace

\bottomrule


\toprule

{} &{0} &{1} &{2} &{3} &{4} &{5} &{6} &{7} &{8} &{9}\\
\hline

{0} &{320} &{0} &{35} &{0} &{1} &{0} &{2} &{0} &{1} &{0}\\
\addlinespace

{1} &{0} &{250} &{3} &{0} &{5} &{0} &{4} &{0} &{0} &{2}\\
\addlinespace

{2} &{4} &{0} &{185} &{1} &{3} &{0} &{0} &{0} &{5} &{0}\\
\addlinespace

{3} &{0} &{0} &{49} &{109} &{0} &{7} &{0} &{0} &{1} &{0}\\
\addlinespace

{4} &{0} &{1} &{60} &{0} &{134} &{0} &{1} &{1} &{0} &{3}\\
\addlinespace

{5} &{2} &{0} &{80} &{1} &{0} &{74} &{0} &{0} &{1} &{2}\\
\addlinespace

{6} &{4} &{0} &{28} &{0} &{2} &{0} &{135} &{0} &{1} &{0}\\
\addlinespace

{7} &{0} &{0} &{31} &{0} &{3} &{0} &{0} &{110} &{0} &{3}\\
\addlinespace

{8} &{4} &{0} &{46} &{7} &{2} &{5} &{0} &{0} &{101} &{1}\\
\addlinespace

{9} &{0} &{0} &{11} &{0} &{7} &{0} &{0} &{2} &{0} &{157}\\

\bottomrule

\end{tabular}
\end{table}

\begin{table}\centering
\begin{tabular}{c c c c c c c c c c c}

\toprule

{} &{0} &{1} &{2} &{3} &{4} &{5} &{6} &{7} &{8} &{9}\\
\hline

{0} &{350} &{0} &{0}&{0} &{4} &{0} &{4} &{0} &{0} &{1}\\
\addlinespace

{1} &{0} &{260} &{0} &{0} &{3} &{0} &{1} &{0} &{0} &{0}\\
\addlinespace

{2} &{14} &{6} &{138} &{11} &{20} &{1} &{3} &{2} &{0} &{3}\\
\addlinespace

{3} &{16} &{3} &{5} &{128} &{2} &{0} &{2} &{1} &{0} &{9}\\
\addlinespace

{4} &{2} &{20} &{3} &{0} &{169} &{0} &{3} &{1} &{0} &{2}\\
\addlinespace

{5} &{55} &{5} &{1} &{40} &{16} &{23} &{10} &{0} &{0} &{10}\\
\addlinespace

{6} &{44} &{2} &{3} &{0} &{3} &{0} &{118} &{0} &{0} &{0}\\
\addlinespace

{7} &{0} &{6} &{1} &{0} &{24} &{0} &{0} &{59} &{0} &{57}\\
\addlinespace

{8} &{27} &{17} &{6} &{20} &{45} &{0} &{3} &{2} &{5} &{41}\\
\addlinespace

{9} &{1} &{14} &{0} &{0} &{36} &{0} &{0} &{1} &{0} &{125}\\

\bottomrule


\toprule

{} &{0} &{1} &{2} &{3} &{4} &{5} &{6} &{7} &{8} &{9}\\
\hline

{0} &{349} &{0} &{1} &{1} &{2} &{0} &{4} &{0} &{1} &{1}\\
\addlinespace

{1} &{0} &{254} &{0} &{0} &{4} &{0} &{5} &{0} &{0} &{1}\\
\addlinespace

{2} &{5} &{0} &{166} &{4} &{9} &{2} &{2} &{1} &{9} &{0}\\
\addlinespace

{3} &{3} &{0} &{7} &{140} &{0} &{12} &{0} &{1} &{1} &{2}\\
\addlinespace

{4} &{0} &{2} &{8} &{0} &{184} &{0} &{1} &{1} &{0} &{4}\\
\addlinespace

{5} &{7} &{0} &{0} &{9} &{3} &{134} &{0} &{1} &{1} &{5}\\
\addlinespace

{6}&{6} &{0} &{4} &{0} &{3} &{3} &{153} &{0} &{1} &{0}\\
\addlinespace

{7} &{0} &{0} &{2} &{0} &{9} &{0} &{0} &{131} &{0} &{5}\\
\addlinespace

{8} &{5} &{2} &{4} &{15} &{2} &{3} &{0} &{1} &{130} &{4}\\
\addlinespace

{9} &{0} &{3} &{0} &{0} &{4} &{1} &{0} &{3} &{3} &{163}\\

\bottomrule


\toprule

{} &{0} &{1} &{2} &{3} &{4} &{5} &{6} &{7} &{8} &{9}\\
\hline

{0} &{320} &{0} &{35} &{0} &{1} &{0} &{2} &{0} &{1} &{0}\\
\addlinespace

{1} &{0} &{250} &{3} &{0} &{5} &{0} &{4} &{0} &{0}&{2}\\
\addlinespace

{2} &{4} &{0} &{185} &{1} &{3} &{0} &{0} &{0} &{5} &{0}\\
\addlinespace

{3} &{0} &{0} &{49} &{109} &{0} &{7} &{0} &{0} &{1} &{0}\\
\addlinespace

{4} &{0} &{1} &{60} &{0} &{134} &{0} &{1} &{1}&{0} &{3}\\
\addlinespace

{5} &{2} &{0} &{80} &{1} &{0} &{74} &{0} &{0} &{1} &{2}\\
\addlinespace

{6} &{4} &{0} &{28} &{0} &{2} &{0} &{135} &{0} &{1} &{0}\\
\addlinespace

{7} &{0} &{0} &{31} &{0} &{3} &{0} &{0} &{110} &{0} &{3}\\
\addlinespace

{8} &{4} &{0} &{46} &{7} &{2} &{5} &{0} &{0} &{101} &{1}\\
\addlinespace

{9} &{0} &{0} &{11} &{0} &{7} &{0} &{0} &{2} &{0} &{157}\\

\bottomrule

\end{tabular}
\end{table}



\doublespacing

[Insert some commentary about the results in these tables.] It is important to note that, although it may not be apparent from the few examples of cost here, using a cost value that is too small risks overfitting the data.

The e1071 package simplifies model selection with the tune() function. The tune() function takes ranges for cost and $\gamma$ and performs ten-fold cross-validation for each possible combination of the two parameters defined in the ranges, and the optimal combination is associated with the lowest cross-validation error. If one is moving from a wide grid to a narrow grid, then the narrow grid can be defined around the optimal values uncovered in the wider grid, and the tune() function can be executed a second time.

This is a good opportunity to highlight one of the more notorious problems with SVMs. The grid search cross-validation can be quite slow and computationally expensive. Since the tune() function performs ten-fold cross validation for every combination of cost and $\gamma$, for even a wide, cursory grid search of five values for each parameter, that means estimating $5 \times 5 \times 10 = 250$ models. For large data sets and large predictor spaces this can be lengthy and computationally prohibitive.

[Insert discussion of why this happens here.]

For demonstration purposes, to avoid this complication, I perform model selection using only one-fourth of the training data, randomly selected, and apply the resulting model to the test data. I begin with broad ranges $\gamma = 10^{-4}, 10^{-3}, 10^{-2}, 10^{-1}, 10^{0}, 10^{1}$ and $cost = 10^{-3}, 10^{-2}, 10^{-1}$, and I define a narrower range around the optimal values found in the first execution. It is possible that I will not converge on a global optimum, but this is for demonstration purposes, so a global optimum is unnecessary.

\singlespacing
\begin{center}
\textbf{Coarse Grid Search}
\end{center}
| $\gamma$ | Cost |  Error   |
|:--------:|:----:|:--------:|
| 0.0001   |0.001 |0.8353660 |
| 0.001    |0.001 |0.8353660 |
| 0.01     |0.001 |0.8353660 |
| 0.1      |0.001 |0.8353660 |
| 1.0      |0.001 |0.8353660 |
| 10.0     |0.001 |0.8353660 |
| 0.0001   |0.01  |0.8353660 |
| 0.001    |0.01  |0.6981565 |
| 0.01     |0.01  |0.7014442 |
| 0.1      |0.01  |0.8353660 |
| 1.0      |0.01  |0.8353660 |
| 10.0     |0.01  |0.8353660 |
| 0.0001   |0.1   |0.7003573 |
| 0.001    |0.1   |0.1531316 |
| 0.01     |0.1   |0.4072329 |
| 0.1      |0.1   |0.7535909 |
| 1.0      |0.1   |0.8353660 |
| 10.0     |0.1   |0.8353660 |

\begin{center}
\textbf{Fine Grid Search}
\end{center}
| $\gamma$ | Cost  |  Error   |
|:--------:|:-----:|:--------:|
| 0.00075  |0.0975 |0.1893292 |
| 0.001    |0.0975 |0.1558548 |
| 0.0025   |0.0975 |0.1344623 |
| 0.005    |0.0975 |0.1690476 |
| 0.00075  |0.1    |0.1832913 |
| 0.001    |0.1    |0.1525641 |
| 0.0025   |0.1    |0.1322705 |
| 0.005    |0.1    |0.1652045 |
| 0.00075  |0.125  |0.1520116 |
| 0.001    |0.125  |0.1350027 |
| 0.0025   |0.125  |0.1190927 |
| 0.005    |0.125  |0.1547739 |

\doublespacing

The lowest cross-validation error is approximately 0.1531, for $\gamma$ and cost of $10^{-3}$ and $10^{-1}$. I conduct a finer grid search around these values: $\gamma = 7.5\times10^{-4}, 1.0\times10^{-3}, 2.5\times10^{-3}, 5.0\times10^{-3}$ and $cost = 9.75\times10^{-2}, 1.00\times10^{-1}, 1.25\times10^{-1}$.

The lowest error in the finer grid search is 0.1191, and the $\gamma$ and cost are $2.5\times10^{-3}$ and $1.25\times10^{-1}$. This is smaller than the wider grid search, so the finer search is successful. The model with the lowest cross-validation error is stored in *zipTune2$best.model*, and this is the model I will use for prediction with the test data.

[Don't forget to apply the model again to the training data for a training error. Should be interesting . . .]

\singlespacing

\begin{center}
\textbf{Model Predictions}
\end{center}

|   | 0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 |
|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|
| 0 |341| 0 | 11| 0 | 1 | 0 | 5 | 0 | 1 | 0 |
| 1 | 0 |254| 0 | 0 | 4 | 0 | 5 | 0 | 0 | 1 |
| 2 | 4 | 0 |165| 5 | 10| 1 | 3 | 1 | 9 | 0 |
| 3 | 6 | 0 | 16|130| 1 | 6 | 1 | 0 | 4 | 2 |
| 4 | 2 | 2 | 18| 0 |163| 1 | 3 | 3 | 0 | 8 |
| 5 | 10| 0 | 19| 6 | 3 |111| 5 | 0 | 2 | 4 |
| 6 | 14| 0 | 6 | 0 | 3 | 3 |144| 0 | 0 | 0 |
| 7 | 0 | 0 | 8 | 0 | 9 | 0 | 0 |122| 0 | 8 |
| 8 | 5 | 2 | 17| 8 | 5 | 5 | 2 | 1 |117| 4 |
| 9 | 1 | 3 | 2 | 0 | 17| 0 | 0 | 7 | 3 |144|

\doublespacing

Correct classifications: 1,691

Misclassification rate = $1 - 1,691/2,007 \approx 0.1574$.

# Conclusion

\begin{thebibliography}{9}

\bibitem{}
Burges, Christopher J.C. (1998). A Tutorial on Support Vector Machines for Pattern Recognition. \textit{Data Mining and Knowledge Discovery}, 2(2), 121-167. doi: 10.1023/A:1009715923555

\bibitem{}
Cortes, Corinna and Vapnik, Vladimir. (1995).

\bibitem{}
Introductory Statistical Learning: with Applications in R

\bibitem{}
Elements of Statistical Learning

\bibitem{}
Hsu, Chih-Wei, et al. A Practical Guide to Support Vector Classification

\bibitem{}
Rockafellar -- Lagrange Optimization

\end{thebibliography}

zipcode data is obtained from: https://web.stanford.edu/~hastie/StatLearnSparsity_files/DATA/zipcode.html

```{r}
citation(package = "e1071")
```

\singlespacing
## R Code Appendix
```{r, echo = TRUE, eval = FALSE}
## Set up working environment
rm(list=ls())
setwd("E:\\MATH\\MATH 6388 -- Machine Learning\\Projects\\Final Project")
install.packages("e1071")

## Accessing and reading data
zipTrain = read.table("zip.train")
zipTest = read.table("zip.test")

## Confirm data read correctly
dim(zipTrain)
str(zipTrain[, 1:10])
zipTrain[1:10, 1:10]

dim(zipTest)
str(zipTest[, 1:10])
zipTest[1:10, 1:10]

## plot a few images
im0 <- matrix(as.numeric(zipTrain[9, 2:257]), nrow = 16, ncol = 16)
im1 <- matrix(as.numeric(zipTrain[8, 2:257]), nrow = 16, ncol = 16)
im2 <- matrix(as.numeric(zipTrain[42, 2:257]), nrow = 16, ncol = 16)
im3 <- matrix(as.numeric(zipTrain[5, 2:257]), nrow = 16, ncol = 16)
im4 <- matrix(as.numeric(zipTrain[4, 2:257]), nrow = 16, ncol = 16)
im5 <- matrix(as.numeric(zipTrain[2, 2:257]), nrow = 16, ncol = 16)
im6 <- matrix(as.numeric(zipTrain[1, 2:257]), nrow = 16, ncol = 16)
im7 <- matrix(as.numeric(zipTrain[4, 2:257]), nrow = 16, ncol = 16)
im8 <- matrix(as.numeric(zipTrain[18, 2:257]), nrow = 16, ncol = 16)
im9 <- matrix(as.numeric(zipTrain[65, 2:257]), nrow = 16, ncol = 16)

par(mfrow=c(2, 5))
image(t(apply(-im0, 1, rev)), col = gray((0:32)/32))  ## this is a 0
image(t(apply(-im1, 1, rev)), col = gray((0:32)/32))  ## this is a 1
image(t(apply(-im2, 1, rev)), col = gray((0:32)/32))  ## this is a 2
image(t(apply(-im3, 1, rev)), col = gray((0:32)/32))  ## this is a 3
image(t(apply(-im4, 1, rev)), col = gray((0:32)/32))  ## this is a 4
image(t(apply(-im5, 1, rev)), col = gray((0:32)/32))  ## this is a 5
image(t(apply(-im6, 1, rev)), col = gray((0:32)/32))  ## this is a 6
image(t(apply(-im7, 1, rev)), col = gray((0:32)/32))  ## this is a 7
image(t(apply(-im8, 1, rev)), col = gray((0:32)/32))  ## this is a 8
image(t(apply(-im9, 1, rev)), col = gray((0:32)/32))  ## this is a 9

## Count of each digit in training/test
table(zipTrain$V1)
table(zipTest$V1)

## Histograms of outcome variables in training/test
par(mfrow=c(1,1))
hist(zipTrain$V1, col = "red") ## outcome variable; notice the concentration at 0 and 1
hist(zipTest$V1, col = "blue")

## Prepare data frames for use by algorithms
library(e1071)
y = as.factor(zipTrain$V1) # output must be a factor for classification
x = as.matrix(zipTrain[, -1])
zipTrain2 = data.frame(y, x)

y2 = as.factor(zipTest$V1) # output must be a factor for classification
x2 = as.matrix(zipTest[, -1])
zipTest2 = data.frame(y2, x2)

## Demonstrate different values of gamma and cost

# Support Vector Classifier
svmfit.lin = svm(y ~ ., data = zipTrain2, kernel = "linear", cost = 1e-4)
fitted.lin = predict(svmfit.lin, zipTest2)
table(y2, fitted.lin)

# Different values of gamma
svmfit.g1 = svm(y ~ ., data = zipTrain2, kernel ="radial", gamma = 1e-4, cost = 1e-1)
fitted.g1 = predict(svmfit.g1, zipTest2)
table(y2, fitted.g1)

svmfit.g2 = svm(y ~ ., data = zipTrain2, kernel ="radial", gamma = 1e-3, cost = 1e-1)
fitted.g2 = predict(svmfit.g2, zipTest2)
table(y2, fitted.g2)

svmfit.g3 = svm(y ~ ., data = zipTrain2, kernel ="radial", gamma = 1e-2, cost = 1e-1)
fitted.g3 = predict(svmfit.g3, zipTest2)
table(y2, fitted.g3)

# Different values of cost
svmfit.c1 = svm(y ~ ., data = zipTrain2, kernel ="radial", gamma = 1, cost = 1e-2)
fitted.c1 = predict(svmfit.g1, zipTest2)
table(y2, fitted.c1)

svmfit.c2 = svm(y ~ ., data = zipTrain2, kernel ="radial", gamma = 1, cost = 1e-1)
fitted.c2 = predict(svmfit.g2, zipTest2)
table(y2, fitted.c2)

svmfit.c3 = svm(y ~ ., data = zipTrain2, kernel ="radial", gamma = 1, cost = 1e0)
fitted.c3 = predict(svmfit.g3, zipTest2)
table(y2, fitted.c3)

## Train the model
# Using only one-fourth of data because the algorithm takes too long
set.seed(101)
a.sample = sample.int(n = nrow(zipTrain2), 
                      size = floor((1/4)*nrow(zipTrain2)), replace = FALSE)
zipTrainSam = zipTrain2[a.sample, ]

# Coarse grid search
zipTune = tune(svm, y ~ ., data = zipTrainSam, kernel = "radial", 
               ranges = list(gamma = 10^(-4:1), cost = 10^(-3:-1)))
summary(zipTune)

# Fine grid search
zipTune2 = tune(svm, y ~ ., data = zipTrainSam, kernel = "radial",
               ranges = list(gamma = c(0.00075, 0.0010, 0.0025, 0.0050), 
                             cost = c(0.0975, 0.1000, 0.1250)))
summary(zipTune2)

## Test the model
zipPred = predict(zipTune2$best.model, newdata = zipTest2)
summary(zipPred)

table(y2, zipPred)
```